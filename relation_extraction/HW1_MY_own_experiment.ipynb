{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1sf1aroj85hvIdCG0NVEZ5NKKuAUWuUsN","timestamp":1671128244580},{"file_id":"1VdmB-7CAuZq-MaFNnblyUJwXfQATe9pz","timestamp":1665003206980}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["# !pip install kaggle\n","\n","# !rm -rf .kaggle/\n","\n","# !mkdir .kaggle\n","# !touch .kaggle/kaggle.json\n","# !chmod 600 ~/.kaggle/kaggle.json\n","# !pip install kaggle -q      # At first, I suspect the kaggle API lose effect so it doesn't have .kaggle folder. (not working)\n","# !rm -rf /root/.kaggle.      # when I created the folder, it says the file or dir already exits\n","# !mkdir /root/.kaggle        # successful\n","# !mv kaggle.json /root/.kaggle/kaggle.json    # not sure if I have to use full destination path, I previously only used /root/.kaggle and it failed. Don't have time to validate this thought.\n","# !ls /root/.kaggle/kaggle.json\n","# !chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"Iq_tcuQ77LPn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!mv .kaggle/kaggle.json /root\n","#!mv .kaggle /root/\n","# !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","# !unzip ./imdb-dataset-of-50k-movie-reviews.zip"],"metadata":{"id":"dcN8GECJkRZD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4KwToMrK7v7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from tqdm import tqdm\n","from collections import Counter"],"metadata":{"id":"0OVvz-8JoWRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)"],"metadata":{"id":"UO-vlmj4sGZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/NLP243/243_HW1/data/hw1_train-1.csv')\n","df.columns = df.columns.str.replace('textstr ', 'review')\n","train_data, val_data = train_test_split(df, test_size=0.2)\n","# train_data.rename(columns = {'textstr':'review'}, inplace = True)"],"metadata":{"id":"lmTMCfBJof3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.head(10)"],"metadata":{"id":"SIU72Pr4Sj9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.columns.tolist())"],"metadata":{"id":"zdn772roUvB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data['review']"],"metadata":{"id":"ykbY1ixdU94Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from spacy.tokenizer import Tokenizer\n","from spacy.lang.en import English\n","nlp = English()\n","# Create a blank Tokenizer with just the English vocab\n","tokenizer = Tokenizer(nlp.vocab)"],"metadata":{"id":"FVNEiexrol57"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens = tokenizer('this is a test.')\n","tokens"],"metadata":{"id":"oIgwGP0xvUVb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = 8_000\n","all_tokens = []\n","for rev in train_data['review']:\n","  tokens = tokenizer(rev)\n","  all_tokens.extend([i.text for i in tokens])"],"metadata":{"id":"5rB7VEKt1ODh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = Counter(all_tokens)\n","tokens, counts = zip(*count.most_common(vocab_size))\n","vocab = {token: idx for idx, token in enumerate(tokens)}\n","vocab['<unk>'] = len(vocab)\n","vocab"],"metadata":{"id":"ZdMvJ3xs8AmV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vocab['<unk>'])\n","print(vocab['relevant'])"],"metadata":{"id":"OleQxm_P8wvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.fillna('none', inplace=True)\n","uniq_labels = train_data['label'].unique()\n","\n","all_labels=[]\n","for line in uniq_labels:\n","  all_labels.extend(line.split())\n","\n","all_labels = list(set(all_labels))\n","\n","print(\"{\")\n","\n","for i in range(len(all_labels)):\n","  print('\\t\"'+all_labels[i]+'\" : ', i ,\" ,\")\n","\n","print(\"}\")"],"metadata":{"id":"d4UPZILqV9YG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_dict = {\n","    \"movie.subjects\" :  0  ,\n","\t\"movie.starring.character\" :  1  ,\n","\t\"movie.gross_revenue\" :  2  ,\n","\t\"movie.initial_release_date\" :  3  ,\n","\t\"movie.production_companies\" :  4  ,\n","\t\"movie.starring.actor\" :  5  ,\n","\t\"person.date_of_birth\" :  6  ,\n","\t\"actor.gender\" :  7  ,\n","\t\"movie.produced_by\" :  8  ,\n","\t\"movie.directed_by\" :  9  ,\n","\t\"movie.rating\" :  10  ,\n","\t\"movie.estimated_budget\" :  11  ,\n","\t\"movie.music\" :  12  ,\n","\t\"movie.language\" :  13  ,\n","\t\"none\" :  14  ,\n","\t\"gr.amount\" :  15  ,\n","\t\"movie.country\" :  16  ,\n","\t\"movie.genre\" :  17  ,\n","\t\"movie.locations\" :  18  \n","}"],"metadata":{"id":"_1vpxcJTbO3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IMDBDataset(Dataset):\n","  def __init__(self, data: pd.DataFrame, vocab, label_dict):\n","    self.data = data\n","    self.vocab = vocab\n","    self.default = self.vocab['<unk>']\n","    self.labels = label_dict\n","\n","  def tokenize(self, text: str):\n","    return [i.text for i in tokenizer(text)]\n","\n","  def encode_tokens(self, tokens):\n","    encoded = [self.vocab.get(token, self.default) for token in tokens]\n","    return torch.tensor(encoded, device=device)\n","\n","  def encode_label(self, label: str):\n","    # encoded = [self.labels.get(token, self.default) for token in label]\n","    # return torch.tensor(encoded, device=device)    \n","    encoded = [label_dict[k] for k in label.split()]\n","    return torch.tensor(encoded, device=device ,dtype=torch.long)\n","  \n","  def __getitem__(self, n: int):\n","    review = self.data['review'].iloc[n]\n","    classes = self.data['label'].iloc[n]\n","    return self.encode_tokens(self.tokenize(review)), self.encode_label(classes)\n","\n","  def __len__(self):\n","    return len(self.data)"],"metadata":{"id":"JeorEKL1pO9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = IMDBDataset(train_data, vocab, label_dict)\n","val_ds = IMDBDataset(val_data, vocab, label_dict)"],"metadata":{"id":"LTam_QwguXgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n","val_loader = DataLoader(val_ds, batch_size=1, shuffle=True)"],"metadata":{"id":"77OMpyO4uaJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(iter(train_loader))"],"metadata":{"id":"ssdjcCD9c_7s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class MLP(nn.Module):\n","#   #a multi-layered perceptron based classifier\n","#     def __init__(self, num_features,out_features):\n","#         \"\"\"\n","#         Args:\n","#             num_features (int): the size of the input feature vector\n","#         \"\"\"\n","#         super(MLP, self).__init__()\n","#         self.fc1 = nn.Linear(in_features=num_features, out_features=64)\n","#         print(\"num f:\", num_features)\n","#         self.fc2 = nn.Linear(in_features=64,out_features=32)\n","#         self.fc3 = nn.Linear(in_features=32,out_features=out_features)\n","\n","#     def forward(self, x_in, apply_softmax=False):\n","#         \"\"\"The forward pass of the classifier\n","        \n","#         Args:\n","#             x_in (torch.Tensor): an input data tensor. \n","#                 x_in.shape should be (batch, num_features)\n","#             apply_softmax (bool): a flag for the sigmoid activation\n","#                 should be false if used with the Cross Entropy losses\n","#         Returns:\n","#             the resulting tensor. tensor.shape should be (batch,)\n","#         \"\"\"\n","#         y_out_1 = torch.relu(self.fc1(x_in))\n","#         y_out_2 = self.fc2(y_out_1)\n","#         y_out = self.fc3(y_out_2)\n","#         return y_out\n","\n","class MLP(nn.Module):\n","  def __init__(self, n_tokens, emb_dim, hidden_dim, output_dim):\n","    super().__init__()\n","    self.embedding = nn.Embedding(n_tokens, emb_dim)\n","    self.fc1 = nn.Linear(emb_dim, hidden_dim)\n","    self.relu = nn.ReLU()\n","    self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","  def forward(self, x):\n","    print(\"shape input\", x.shape)\n","    # x: Tensor([[0, 1, 2, 5, 100, 3, 6]]), shape [B, seq_len]\n","    # embedding.weight:\n","    # 0:       [ 0.3, 0.5, ..., 0.7]\n","    #          ...\n","    # n_token: [ 1.0, 0.8, ..., 0.8]\n","    # \n","    # embedded = embedding(0) + embedding(1) + ... + embedding(6)\n","    embedded = self.embedding(x)\n","    print(\"shape after embedding\", embedded.shape)\n","    # embedded: Tensor([[0.4, 0.2, ..., -0.9]]), shape [B, emb_dim]\n","    hidden1 = self.fc1(embedded)\n","    print(\"shape after hidden1\", hidden1.shape)\n","    hidden2 = self.relu(hidden1)\n","    print(\"shape after hidden2\", hidden2.shape)\n","    y_out = self.fc2(hidden2)\n","    y_out = y_out.view(-1,output_dim)\n","    print(\"shape of y_out\",y_out.shape)\n","    return y_out\n","    "],"metadata":{"id":"QK2k1sMMxUgp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = vocab_size + 1\n","emb_dim = 100\n","hidden_dim = 200\n","output_dim = 19\n","model = MLP(vocab_size + 1, 100, 200, 19).to(device)"],"metadata":{"id":"g-y-tE1X1FIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(loader, model, optimizer, loss_fn):\n","  model.train()\n","  losses = []\n","  pbar = tqdm(loader)\n","  for x, y in pbar:\n","    print(\"Input shape\", x.shape)\n","    print(\"output shape\", y.shape)\n","    optimizer.zero_grad()\n","    logits = model(x)\n","    print(\"logits shape\", logits.shape)\n","    # print(\"y shape\", y.shape)\n","    loss = loss_fn(logits, y)\n","    pbar.set_postfix({'loss': loss.item()})\n","    losses.append(loss.item())\n","\n","    loss.backward()  # calculate gradients for w/b\n","    optimizer.step()  # update weights according to optimizer rules\n","  return sum(losses) / len(losses)\n","\n","\n","def evaluate(loader, model, loss_fn, score_fn):\n","  model.eval()\n","  predictions = []\n","  labels = []\n","  for x, y in tqdm(loader):\n","    logits = model(x)\n","    loss = loss_fn(logits, y)\n","    pred = torch.argmax(logits, dim=-1)\n","    predictions.append(pred.numpy())\n","    labels.append(y.numpy())\n","  score = score_fn(labels, predictions)\n","  return score"],"metadata":{"id":"4m9JabMa2AUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aH287_T0D9jY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d4vq_dIuD-U8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.train()\n","losses = []\n","pbar = tqdm(loader)\n","for x, y in pbar:\n","  print(\"Input shape\", x.shape)\n","  print(\"output shape\", y.shape)\n","  optimizer.zero_grad()\n","  logits = model(x)\n","  print(\"logits shape\", logits.shape)\n","  # print(\"y shape\", y.shape)\n","  loss = loss_fn(logits, y)\n","  pbar.set_postfix({'loss': loss.item()})\n","  losses.append(loss.item())\n","\n","  loss.backward()  # calculate gradients for w/b\n","  optimizer.step()  # update weights according to optimizer rules"],"metadata":{"id":"Fbtw0-nwsX8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PNYTjoJjD-vI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n","loss_fn = nn.BCEWithLogitsLoss()\n","score_fn = accuracy_score\n","n_epochs = 3\n","best_acc = 0\n","for epoch in range(n_epochs):\n","  avg_loss = train(train_loader, model, optimizer, loss_fn)\n","  print('train loss: ', avg_loss)\n","  accuracy = evaluate(val_loader, model, loss_fn, score_fn)\n","  print('val accuracy: ', accuracy)\n","  if accuracy > best_acc and accuracy > 0.7:\n","    torch.save(model.state_dict(), f'best-model.pt')"],"metadata":{"id":"oCY0lQYF5oHV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s1 = torch.randint(0, 10, (1, 6))\n","pad = torch.zeros(size=(1, 4)) - 1\n","s3 = torch.cat([s1, pad], dim=1)\n","s2 = torch.randint(0, 10, (1, 10))"],"metadata":{"id":"ZKvgXjIK6qtp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cat([s1], dim=0)"],"metadata":{"id":"bAMmr0uaDxXL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.preprocessing import OneHotEncoder"],"metadata":{"id":"3Nfkt7arD1iO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlb = MultiLabelBinarizer()"],"metadata":{"id":"zETITy4tFXvf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlb.fit([['apple', 'banana', 'orange']])"],"metadata":{"id":"hfmi7OZlFaUP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlb.transform([['apple', 'banana']])"],"metadata":{"id":"WP5_ScOOFcan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ohe = OneHotEncoder()"],"metadata":{"id":"TtqZPvRQFfQa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6MSEMsgFFp09"},"execution_count":null,"outputs":[]}]}
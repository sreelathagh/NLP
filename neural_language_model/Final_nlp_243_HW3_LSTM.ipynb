{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Install datasets"],"metadata":{"id":"zeZG_Yql9uvP"}},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"aHi0d3An9tsP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Import all libs"],"metadata":{"id":"D6ijVX48DuBl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZgxDDb32USi"},"outputs":[],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from collections import Counter\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import torch.nn as nn\n","import re\n","import numpy as np"]},{"cell_type":"markdown","source":["#device"],"metadata":{"id":"HDXm5UvMDyZj"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)"],"metadata":{"id":"-HnU8eI-QmML"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Installing penn tree dataset"],"metadata":{"id":"pYH0qMlgD-v4"}},{"cell_type":"code","source":["from datasets import load_dataset\n","ptb = load_dataset('ptb_text_only')"],"metadata":{"id":"R6de_lOY2yCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ptb #penn tree "],"metadata":{"id":"0qHpDtuj3Nr2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Tokenization"],"metadata":{"id":"29VbJIVOEE-4"}},{"cell_type":"code","source":["#tokenization\n","all_train_tokens = []\n","train_tokens = []\n","for i in range(0,len(ptb['train'])):  #tokenizing train tokens, tokens are created with slpit() and hyphanated words are split and then tokenized\n","  tokens = re.split(' ', '<sep>'+' '+ ptb['train'][i]['sentence'].replace('-', ' '))  #adding <sep> token at the beginning of each sentence, and splitting the sentence with ' ' delimiter\n","  all_train_tokens.extend([word for word in tokens])\n","  train_tokens.append(tokens)\n","\n","\n","all_val_tokens = []\n","for i in range(0,len(ptb['validation'])):  #tokenizing validation tokens, tokens are created with slpit() and hyphanated words are split and then tokenized\n","  tokens = re.split(' ', '<sep>'+' '+ ptb['validation'][i]['sentence'].replace('-', ' ')) #adding <sep> token at the beginning of each sentence, and splitting the sentence with ' ' delimiter\n","  all_val_tokens.extend([word for word in tokens])\n","\n","all_test_tokens = []\n","for i in range(0,len(ptb['test'])):  #tokenizing test tokens\n","  tokens = re.split(' ',ptb['test'][i]['sentence']) \n","  all_test_tokens.extend([word for word in tokens])\n","\n","all_tokens = all_train_tokens + all_val_tokens #combining all the trining and validation tokens \n","\n","len(all_train_tokens), len(all_val_tokens),len(all_tokens), all_tokens[0], len(all_test_tokens)"],"metadata":{"id":"AId5tCjUKVph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#creating the Vocab for train and validation data"],"metadata":{"id":"ta6ab9q0EOkL"}},{"cell_type":"code","source":["vocab_size = len(all_tokens)  #creating the vocab\n","count = Counter(all_tokens)\n","tokens, counts = zip(*count.most_common(vocab_size))\n","vocab = {token: idx for idx, token in enumerate(tokens)} #creating a vocab dictionary\n","len(vocab), vocab['<unk>'],  vocab['<sep>']"],"metadata":{"id":"n92GUoQtQtk3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#encoding all tokens(train and validation) from the corpus"],"metadata":{"id":"AXX8HCVZCE4-"}},{"cell_type":"code","source":["def encode_tokens(tokens):\n","  encoded = [vocab.get(token, -1) for token in tokens]\n","  return encoded\n","\n","def encode_test_tokens(tokens):\n","  encoded = [vocab.get(token, 1) for token in tokens]\n","  return encoded\n","\n","train_data = encode_tokens(all_train_tokens)\n","val_data = encode_tokens(all_val_tokens)\n","test_data = encode_test_tokens(all_test_tokens)\n","len(train_data), len(val_data), len(test_data)"],"metadata":{"id":"a5Fksu22_pS9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Creating the data chunks with some fixed lengths"],"metadata":{"id":"aSdWJxDwNi7c"}},{"cell_type":"code","source":["chunk_size = 50\n","x_train = []\n","y_train = []\n","for i in range(0, len(train_data), chunk_size):\n","  x_train.append(train_data[i:i+chunk_size])\n","\n","for i in range(1, len(train_data), chunk_size):\n","  y_train.append(train_data[i:i+chunk_size])\n","\n","x_val = []\n","y_val = []\n","for i in range(0, len(val_data), chunk_size):\n","  x_val.append(val_data[i:i+chunk_size])\n","\n","for i in range(1, len(val_data), chunk_size):\n","  y_val.append(val_data[i:i+chunk_size])\n","\n","x_test = []\n","y_test = []\n","for i in range(0, len(test_data), chunk_size):\n","  x_test.append(test_data[i:i+chunk_size])\n","\n","for i in range(1, len(test_data), chunk_size):\n","  y_test.append(test_data[i:i+chunk_size])\n","\n","del x_train[(len(x_train)-1):]\n","del y_train[(len(y_train)-1):]\n","del x_val[(len(x_val)-1):]\n","del y_val[(len(y_val)-1):]\n","del x_test[(len(x_test)-1):]\n","del y_test[(len(y_test)-1):]\n","\n","len(x_train), len(y_train), len(x_val), len(y_val), len(x_train[0]), len(y_train[0]), len(x_val[0]), len(y_val[0]), len(x_train[18710]), len(y_train[18710]), len(x_val[1480]), len(y_val[1480]), len(x_test), len(y_test)\n"],"metadata":{"id":"hIZy93kVASxd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#creating the embeddings"],"metadata":{"id":"lbOTHY5xHRwC"}},{"cell_type":"code","source":["class PTBDataset(Dataset):\n","  def __init__(self, x: list, y: list):\n","    self.x = x\n","    self.y = y\n","\n","  def encode_tokens(self, tokens):  #creating tensors for train and validation data\n","    return torch.tensor(tokens, device=device)\n","  \n","  def __getitem__(self, n: int):\n","    ip_seq = self.x[n]\n","    op_seq = self.y[n]\n","    return self.encode_tokens(ip_seq), self.encode_tokens(op_seq)\n","\n","  def __len__(self):\n","    return len(self.x)"],"metadata":{"id":"qk5epzBO0rM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = PTBDataset(x_train, y_train) #creating training dataset\n","val_ds = PTBDataset(x_val, y_val) #creating validation dataset\n","test_ds = PTBDataset(x_test, y_test)"],"metadata":{"id":"xI3tNvxoawcY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Hyper meters\n","hidden_size = 256\n","n_epochs = 20\n","learning_rate = 0.02\n","embedding_size = 100\n","vocab_size = len(vocab)\n","batch_size = 1\n","num_layers = 2"],"metadata":{"id":"RRl2v7wOztzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True) #Dataloader for training data\n","val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)  #Dataloader for validation data\n","test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)  #Dataloader for validation data"],"metadata":{"id":"bogQVurpvHMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(iter(test_loader)) #just checking"],"metadata":{"id":"wm1QZ0h09iyl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Definition"],"metadata":{"id":"pTmpdByBPbVu"}},{"cell_type":"code","source":["class LMLSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, batch_size):\n","                \n","        super(LMLSTM, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","        self.batch_size = batch_size\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        # self.dropout = nn.Dropout(dropout_rate)\n","        self.linear = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, input):\n","        input_embedding = self.embedding(input)\n","        #print(input_embedding.shape)\n","        batch_size = input_embedding.size(0)\n","        #print(batch_size)\n","        hidden = self.init_hidden(batch_size)\n","        #print(hidden.shape)\n","        rnn_out, hidden = self.lstm(input_embedding, hidden)\n","        #print(rnn_out.shape)\n","        affine_out = self.linear(torch.squeeze(rnn_out, 0)) #affine transformation\n","        affine_out = affine_out.view(-1,vocab_size)\n","        #print(affine_out.shape)\n","\n","        return F.log_softmax(affine_out)\n","\n","    def init_hidden(self, batch_size):\n","        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n","        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n","        return hidden, cell\n","\n","model = LMLSTM(vocab_size, embedding_size, hidden_size, num_layers, batch_size)\n","model.cuda() #-> model to devise"],"metadata":{"id":"QV2UgrLRppKH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class TagRNN(nn.Module):\n","#     def __init__(self, vocab_size, hidden_size, vocab_n, num_layers):\n","#         super(TagRNN, self).__init__()\n","#         self.vocab_size = vocab_size\n","#         self.hidden_size = hidden_size\n","#         self.vocab_n = vocab_n\n","#         self.batch_size = 64\n","#         self.num_layers = num_layers\n","#         self.embedding = nn.Embedding(self.vocab_size, self.hidden_size) #embedding layer\n","#         self.dropout = nn.Dropout(p=0.2)\n","\n","#         self.rnn = nn.GRU(self.hidden_size, hidden_size=self.hidden_size,\n","#                           bidirectional=False,\n","#                           num_layers=num_layers, batch_first=True) #Applying a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n","\n","#         self.linear = nn.Linear(hidden_size, vocab_size) #output layer\n","\n","#     def forward(self, input):\n","#         input_embedding = self.embedding(input)\n","#         #print(input_embedding.shape)\n","#         batch_size = input_embedding.size(0)\n","#         #print(batch_size)\n","#         hidden = self.init_hidden(batch_size)\n","#         #print(hidden.shape)\n","#         rnn_out, _ = self.rnn(input_embedding, hidden)\n","#         #print(rnn_out.shape)\n","#         affine_out = self.linear(torch.squeeze(rnn_out, 0)) #affine transformation\n","#         #print(affine_out.shape)\n","\n","#         return F.log_softmax(affine_out)\n","\n","#     def init_hidden(self, batch_size):\n","#         # This method generates the first hidden state of zeros which we'll use in the forward pass\n","#         hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n","#          # We'll send the tensor holding the hidden state to the device we specified earlier as well\n","#         return hidden\n","\n","# model = TagRNN(vocab_size, hidden_size, vocab_n, 2)\n","# model.cuda() #-> model to devise"],"metadata":{"id":"AE27wcCbPWw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(loader, model, optimizer, loss_fn):\n","  model.train()\n","  losses = []\n","  pbar = tqdm(loader)\n","  for x, y in pbar:\n","    optimizer.zero_grad()\n","    logits = model(x)\n","    # print(\"logits: \", logits.shape)\n","    # y = torch.squeeze(y,0)\n","    # print(\"y: \", y.shape)\n","    loss = loss_fn(logits, y.view(-1))\n","    pbar.set_postfix({'loss': loss.item()})\n","    losses.append(loss.item())\n","    loss.backward()  # calculate gradients for w/b\n","    optimizer.step()  # update weights according to optimizer rules\n","  return sum(losses) / len(losses)\n","\n","\n","def evaluate(loader, model, loss_fn):\n","  model.eval()\n","  predictions = []\n","  labels = []\n","  losses = []\n","  pbar = tqdm(loader)\n","  for x, y in pbar:\n","    logits = model(x)\n","    y = torch.squeeze(y,0)\n","    loss = loss_fn(logits, y.view(-1))\n","    pbar.set_postfix({'loss': loss.item()})\n","    losses.append(loss.item())    \n","  return sum(losses) / len(losses)\n","\n","def test_fn(loader, model, loss_fn):\n","  model.eval()\n","  losses = []\n","  pbar = tqdm(loader)\n","  for x, y in pbar:\n","    logits = model(x)\n","    y = torch.squeeze(y,0)\n","    loss = loss_fn(logits, y.view(-1))\n","    pbar.set_postfix({'loss': loss.item()})\n","    losses.append(loss.item())    \n","  return sum(losses) / len(losses)"],"metadata":{"id":"ZOnImO4WLIEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15], gamma=0.1)\n","loss_fn = nn.CrossEntropyLoss()#nn.NLLLoss()\n","# score_fn = accuracy_score\n","avg_loss_list = []\n","evalloss_list = []\n","testloss_list = []\n","train_pp = []\n","val_pp = []\n","test_pp = []\n","n_epochs = n_epochs\n","best_acc = 0\n","for epoch in range(n_epochs):\n","  avg_loss = train(train_loader, model, optimizer, loss_fn)\n","  avg_loss_list.append(avg_loss)\n","  print('Tarin Loss: ', avg_loss)\n","  train_perplexity  = torch.exp(torch.tensor(avg_loss))\n","  train_pp.append(train_perplexity)\n","  print('Train Perplexity:', train_perplexity) \n","  scheduler.step()\n","  eval_loss = evaluate(val_loader, model, loss_fn)\n","  evalloss_list.append(eval_loss)\n","  print('Validation Loss: ', eval_loss)\n","  valid_perplexity  = torch.exp(torch.tensor(eval_loss))\n","  print('Validation Perplexity:', valid_perplexity) \n","  val_pp.append(valid_perplexity)\n","  # test_loss = test_fn(test_loader, model, loss_fn)\n","  # testloss_list.append(test_loss)\n","  # print('Test Loss: ', test_loss)\n","  # test_perplexity  = torch.exp(torch.tensor(test_loss))\n","  # print('Test Perplexity:', test_perplexity)  \n","  # test_pp.append(test_perplexity) \n","\n","\n","fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n","\n","# Plot loss values over the epochs\n","ax1 = ax[0]\n","ax1.set_title(\"Train, Validation Losses\")\n","ax1.plot(avg_loss_list, color=\"red\", label=\"Train Loss\")\n","ax1.plot(evalloss_list, color=\"green\", label=\"Validation Loss\")\n","ax1.legend()\n","\n","# Plot accuracies over the epochs\n","ax2 = ax[1]\n","ax2.set_title(\"Train, Validation Perplexities\")\n","ax2.plot(train_pp, color=\"red\", label=\"Train Perplexity\")\n","ax2.plot(val_pp, color=\"green\", label=\"Validation Perplexity\")\n","ax2.legend()\n","\n","plt.show()"],"metadata":{"id":"wLijdd7WNFja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  test_loss = test_fn(test_loader, model, loss_fn)\n","  testloss_list.append(test_loss)\n","  print('Test Loss: ', test_loss)\n","  test_perplexity  = torch.exp(torch.tensor(test_loss))\n","  print('Test Perplexity:', test_perplexity)  \n","  test_pp.append(test_perplexity) \n","\n","  fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n","\n","# Plot loss values over the epochs\n","ax1 = ax[0]\n","ax1.set_title(\"Test Loss\")\n","ax1.plot(testloss_list, color=\"blue\", label=\"Test Loss\")\n","ax1.legend()\n","\n","# Plot accuracies over the epochs\n","ax2 = ax[1]\n","ax2.set_title(\"Test Perplexity\")\n","ax2.plot(test_pp, color=\"blue\", label=\"Test Perplexity\")\n","ax2.legend()\n","\n","plt.show()"],"metadata":{"id":"bBxmsjTXO_Yx"},"execution_count":null,"outputs":[]}]}
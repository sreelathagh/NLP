{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ipE7EMngol0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXiLtzJds1Ye"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "jNWL4xoeibHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cb=pd.read_csv(\"/content/drive/MyDrive/clickbait/clickbait_data\", on_bad_lines='skip', names=[\"headline\", \"label\"])\n",
        "df_ncb=pd.read_csv(\"/content/drive/MyDrive/clickbait/non_clickbait_data\", on_bad_lines='skip',names=[\"headline\", \"label\"])\n",
        "df_cb[\"label\"]=[\"click bait\"]*len(df_cb)\n",
        "df_ncb[\"label\"]=[\"non click bait\"]*len(df_ncb)\n",
        "frames = [df_cb, df_ncb]\n",
        "df=pd.concat(frames)\n",
        "percent_clickbait=(len(df[df[\"label\"]==\"click bait\"])/len(df))*100\n",
        "percent_clickbait\n",
        "print(f\"The percentage of clickbait data in the entire dataset:{round(percent_clickbait,2)}%\")"
      ],
      "metadata": {
        "id": "dfeQDhG3tD2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb = {\"non click bait\":0,\"click bait\":1}\n",
        "df['label']=df['label'].map(cb)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "X48RzEJVaYFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = train_test_split(df, test_size=0.2)\n",
        "print('Train data size={}, Validation data size={}'.format(len(train_data), len(val_data)))"
      ],
      "metadata": {
        "id": "65vJg406it4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "# Create a blank Tokenizer with just the English vocab\n",
        "tokenizer = Tokenizer(nlp.vocab)"
      ],
      "metadata": {
        "id": "n_ggZe54i0O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20_000\n",
        "all_tokens = []\n",
        "for review in tqdm(train_data['headline']):\n",
        "  tokens = tokenizer(review)\n",
        "  all_tokens.extend([i.text for i in tokens])"
      ],
      "metadata": {
        "id": "7iGfdB5Vi7_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = Counter(all_tokens)\n",
        "tokens, counts = zip(*count.most_common(vocab_size))\n",
        "vocab = {token: idx for idx, token in enumerate(tokens)}\n",
        "vocab['<unk>'] = len(vocab)"
      ],
      "metadata": {
        "id": "RjY12SzijEBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab['<unk>'])\n",
        "print(vocab['I'])"
      ],
      "metadata": {
        "id": "N37XDKxCjGei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data.head(10)"
      ],
      "metadata": {
        "id": "LIXhD3QfjzUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_maxlen(data):\n",
        "  maxlen = 0\n",
        "  for ex in train_data['headline']:\n",
        "    max_len_tmp = (len(ex))\n",
        "    if max_len_tmp > maxlen:\n",
        "      maxlen = max_len_tmp\n",
        "  return maxlen\n",
        "\n",
        "max_len_tr = get_maxlen(train_data)\n",
        "max_len_val = get_maxlen(val_data)\n",
        "max_len = max(max_len_tr, max_len_val)\n",
        "print(max_len)\n",
        "print(type(train_data))"
      ],
      "metadata": {
        "id": "BuRIotEO92oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "vocab_size=vocab_size+1\n",
        "embedding_size=100\n",
        "hidden_size=64\n",
        "num_layers=2\n",
        "out_dim=2\n",
        "batch_size = 32\n",
        "max_len=max_len\n",
        "n_epochs=5\n",
        "num_classes=2"
      ],
      "metadata": {
        "id": "ldlmVzGB8kM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class clickbaitDataset(Dataset):\n",
        "  def __init__(self, data: pd.DataFrame, vocab,max_len):\n",
        "    self.data = data\n",
        "    self.vocab = vocab\n",
        "    self.default = self.vocab['<unk>']\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def tokenize(self, text: str):\n",
        "    return [i.text for i in tokenizer(text)]\n",
        "\n",
        "  def encode_tokens(self, tokens):\n",
        "    encoded = [self.vocab.get(token, self.default) for token in tokens]\n",
        "    encoded += [0 for _ in range(self.max_len-len(tokens))]\n",
        "    return torch.tensor(encoded, device=device)\n",
        "\n",
        "  def encode_label(self, label: str):\n",
        "    return torch.tensor(0, device=device) if label == 0 else torch.tensor(1, device=device)\n",
        "  \n",
        "  def __getitem__(self, n: int):\n",
        "    headline = self.data['headline'].iloc[n]\n",
        "    label = self.data['label'].iloc[n]\n",
        "    return self.encode_tokens(self.tokenize(headline)), self.encode_label(label)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "train_ds = clickbaitDataset(train_data, vocab, max_len=max_len)\n",
        "val_ds = clickbaitDataset(val_data, vocab,  max_len=max_len)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "l73aDB-xjLyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "id": "JCZtrg0pkjW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self,  vocab_size, embedding_size, hidden_size, out_dim, batch_size):\n",
        "        super().__init__()\n",
        "        self.num_layers = 2\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_dim = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_heads = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size)\n",
        "        self.nonlinear = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(hidden_size, out_dim)\n",
        "        self.attention = nn.MultiheadAttention(hidden_size, self.num_heads)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        batch_size = embedded.size(0)\n",
        "        input, hidden = self.lstm(embedded) #LSTM layer\n",
        "        attention_out, _ = self.attention(input,input,input) #Added the Multi-headed attention layer.\n",
        "        attention_out = attention_out[:, -1]\n",
        "        out=self.fc(attention_out)\n",
        "        out= self.nonlinear(out)\n",
        "        out=out[:,-1]\n",
        "        return  out\n",
        "\n",
        "model = LSTM(vocab_size, embedding_size, hidden_size, out_dim, batch_size)\n",
        "loss_fn = nn.BCEWithLogitsLoss(reduction=\"sum\") #BCELoss\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "jw8nhL6Nl7HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = []\n",
        "valid_loss = []\n",
        "for epoch in range(n_epochs):\n",
        "  model.train()\n",
        "  avg_loss = 0.  \n",
        "  pbar = tqdm(train_loader)\n",
        "  for i, (x_batch, y_batch) in enumerate(pbar):\n",
        "    y_pred = model(x_batch) #Forwardpass\n",
        "    loss = loss_fn(y_pred, y_batch.float()) # calculate loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    avg_loss += loss.item() / len(train_loader)\n",
        "  model.eval()   # Model Eval       \n",
        "  avg_val_loss = 0.\n",
        "  val_preds = np.zeros((len(val_ds), num_classes))\n",
        "  pbar = tqdm(val_loader)\n",
        "  for i, (x_batch, y_batch) in enumerate(pbar):\n",
        "    y_pred = model(x_batch)\n",
        "    avg_val_loss += loss_fn(y_pred, y_batch.float()).item() / len(val_loader)\n",
        "    val_preds[i * batch_size:(i+1) * batch_size] = y_pred.cpu().data.numpy().argmax() #collecting predictions for F1-score\n",
        "  train_loss.append(avg_loss)\n",
        "  valid_loss.append(avg_val_loss)\n",
        "  print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}'.format(\n",
        "              epoch + 1, n_epochs, avg_loss, avg_val_loss))\n"
      ],
      "metadata": {
        "id": "oamPpyEWsMWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "y_true = [x for x in val_data['label']]\n",
        "y_pred = [x for x in val_preds.argmax(axis=1)]\n",
        "print(classification_report(y_true, y_pred))\n",
        "f1_score(y_true, y_pred, average='macro')"
      ],
      "metadata": {
        "id": "eW4IafPdvYbm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
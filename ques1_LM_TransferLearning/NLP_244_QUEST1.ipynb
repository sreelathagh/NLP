{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1CYyJ-fX843"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/NLP244/NLP244-quest1 -> "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/NLP244/QUEST1/NLP244-quest1"
      ],
      "metadata": {
        "id": "XE5DeZcR5QiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "XZl6iOxg6S8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Nc-tD5nC6VOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd data"
      ],
      "metadata": {
        "id": "lhP_wZIt7AEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python prepare.py"
      ],
      "metadata": {
        "id": "4If_Qjz67DhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./"
      ],
      "metadata": {
        "id": "8tGuTlKY7KLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "hcpCwtX4TBpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../"
      ],
      "metadata": {
        "id": "V7lf8aiZTGfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 -R main.py utils.py data.py rnnlm.py run_tentimes.sh"
      ],
      "metadata": {
        "id": "OFlWvcgITH2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python utils.py"
      ],
      "metadata": {
        "id": "SjHdtK4OTQ9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python data.py"
      ],
      "metadata": {
        "id": "qtd81PaZTim0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python rnnlm.py"
      ],
      "metadata": {
        "id": "Knm67WzGTkc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "id": "XiSTtWsZshaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash run_tentimes.sh"
      ],
      "metadata": {
        "id": "IAbB9ap5Arhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/drive/MyDrive/NLP244/QUEST1/NLP244-quest1/data/wikitext-2/wiki.train.tokens\"\n",
        "tr_tokens=[]\n",
        "with open(path, \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        words = line.split()\n",
        "        tr_tokens.extend(words)\n",
        "tr_tokens.count(\"<unk>\")  \n",
        "len(tr_tokens) \n",
        "per_unk_tokens = round((tr_tokens.count(\"<unk>\") /  len(tr_tokens)) * 100 ,2)\n",
        "print(\"total tokens' count: {}\\n<unk> token count: {}\\n Percentage of <unk> token in the tarining data {}%\".format(len(tr_tokens), tr_tokens.count(\"<unk>\"), per_unk_tokens))"
      ],
      "metadata": {
        "id": "8olpsacdTtG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "total tokens' count: 2051910\n",
        "\n",
        "unk token count: 54625\n",
        "\n",
        " Percentage of unk token in the tarining data 2.66%"
      ],
      "metadata": {
        "id": "iqQcx_6EWK2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/drive/MyDrive/NLP244/QUEST1/NLP244-quest1/data/wikitext-2-mini/wiki.train.tokens\"\n",
        "tr_tokens=[]\n",
        "with open(path, \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        words = line.split()\n",
        "        tr_tokens.extend(words)\n",
        "tr_tokens.count(\"<unk>\")  \n",
        "len(tr_tokens) \n",
        "per_unk_tokens = round((tr_tokens.count(\"<unk>\") /  len(tr_tokens)) * 100 ,2)\n",
        "print(\"total tokens' count: {}\\n<unk> token count: {}\\n Percentage of <unk> token in the tarining data {}%\".format(len(tr_tokens), tr_tokens.count(\"<unk>\"), per_unk_tokens))"
      ],
      "metadata": {
        "id": "LX6z45S2m6Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "total tokens' count: 6464\n",
        "\n",
        "unk token count: 203\n",
        "\n",
        " Percentage of unk token in the tarining data 3.14%"
      ],
      "metadata": {
        "id": "3DNkJ-8ZnNL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the GloVe embeddings into memory\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.array(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Build the vocabulary and prepare the text samples\n",
        "tokenizer = Tokenizer()\n",
        "texts = [ ... ] # List of text samples\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "Phv06KoUYaTf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
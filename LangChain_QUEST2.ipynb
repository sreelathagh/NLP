{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9K8YXqT3QFctxWM6xc22I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreelathagh/NLP/blob/main/LangChain_QUEST2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ws9Oq20mNPWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238a365c-71c0-4052-ba30-0df531b493c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.8/dist-packages (0.0.81)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.8/dist-packages (from langchain) (2.25.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.8/dist-packages (from langchain) (3.8.3)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain) (1.4.46)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain) (1.21.6)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.8/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain) (1.10.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from langchain) (8.1.0)\n",
            "Requirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.8/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain) (4.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.8/dist-packages (0.26.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env OPENAI_API_KEY=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"
      ],
      "metadata": {
        "id": "AKrztyEkQNln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2056e25e-be26-4128-9eaa-2089aa00ba63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OPENAI_API_KEY=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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "mQNXrHFbVEQo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample Example\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2,temperature=0.8, max_tokens=256)\n",
        "response=llm.generate([\"Tell me a joke.\"])\n",
        "response"
      ],
      "metadata": {
        "id": "1EcGI_QrVJXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2d2934-7e8e-477b-8ccb-26957bea808f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\nQ: What do you call a bear with no teeth? \\nA: A gummy bear!', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 43, 'total_tokens': 48, 'prompt_tokens': 5}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question:4"
      ],
      "metadata": {
        "id": "La_z3nWr40ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2,temperature=0.7, max_tokens=1024)\n",
        "prompt_template_names=\"generate some sweet and unique 10 baby girl {name} from India?\"\n",
        "prompt = PromptTemplate(input_variables=[\"name\"], template=prompt_template_names)\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print(chain_1.run('name'))"
      ],
      "metadata": {
        "id": "h7E3V5EWoyUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29aa124a-1e20-4e18-f54f-67e798be6f4e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Aanya\n",
            "2. Aadhya\n",
            "3. Diya\n",
            "4. Archi\n",
            "5. Jiya\n",
            "6. Aashi\n",
            "7. Pari\n",
            "8. Aradhya\n",
            "9. Meera\n",
            "10. Vedika\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question:5"
      ],
      "metadata": {
        "id": "xgZuTbNl46yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name=\"text-davinci-003\",temperature=0.5, max_tokens=1024)\n",
        "prompt_template_bio=\"Create a made up biography of {name} from {origin}\"\n",
        "prompt_2 = PromptTemplate(input_variables=[\"name\", \"origin\"], template=prompt_template_bio)\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
        "\n",
        "print(chain_2.run({\"name\":\"Meera\",\"origin\":\"origin\"}))"
      ],
      "metadata": {
        "id": "jZoBSUWZKpDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a671a12c-f821-41d0-de5c-662ca48c36be"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Meera Patel was born in India in 1991 and moved to the United States with her family when she was only five years old. Growing up in a small town in Ohio, she was surrounded by a diverse group of people that helped shape her into the person she is today.\n",
            "\n",
            "Meera was always a bright student, and she excelled in her studies, eventually earning a degree in Computer Science from Ohio State University. After college, she moved to San Francisco to pursue a career in software engineering.\n",
            "\n",
            "In San Francisco, Meera was exposed to a variety of cultures and experiences, which helped her to develop her own unique style and outlook on life. She found a passion for exploring the world and learning about different cultures, and she was always eager to share her knowledge and perspective with others.\n",
            "\n",
            "Meera’s love of travel eventually led her to the world of virtual reality. She was fascinated by the potential of VR to bring people together and allow them to explore new places and experiences. She began to develop her own VR projects, and soon she was creating immersive experiences that allowed people to explore distant lands and experience different cultures.\n",
            "\n",
            "Today, Meera is a successful entrepreneur and VR developer. Her work has been featured in prestigious publications and she is often invited to speak at conferences and events. She continues to explore the world and share her knowledge and experiences with others.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question:6"
      ],
      "metadata": {
        "id": "4Xl64xAj75y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.base import Chain\n",
        "from typing import Dict, List\n",
        "\n",
        "class NameandBio(Chain):\n",
        "    chain_1: LLMChain\n",
        "    chain_2: LLMChain\n",
        "\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
        "        return list(all_input_vars)\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> Dict[str, str]:\n",
        "      return ['Name and Bio']\n",
        "\n",
        "    def _call(self, inputs1: Dict[str, str]) -> List[str]: \n",
        "        output1 = self.chain_1.run(inputs1['name'])\n",
        "        list_nb=[]\n",
        "        for i in range(0,10):\n",
        "          output2 = self.chain_2.run({inputs1['name']:'name',inputs1['origin']:'origin'})\n",
        "          list_nb.append({output2})\n",
        "        return {'Name and Bio': list_nb}\n",
        "\n",
        "concat_chain = NameandBio(chain_1=chain_1, chain_2=chain_2)\n",
        "concat_output = concat_chain.run({'name':'name', 'origin':'origin'})\n",
        "print(\"Baby_name:Biography\", concat_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWckNwNZEc6Z",
        "outputId": "9f1305fc-e0e8-4944-b28c-58f4f8829ff7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baby_name:Biography [{'\\n\\nJohn Smith:\\nJohn Smith was born in New York City in 1982. He grew up in a loving family, with two younger siblings. His parents were hardworking and instilled a strong work ethic in him from a young age. John excelled in school, and was accepted into an Ivy League university. After graduating, he went on to become a successful businessman, working in finance and investments. He married his college sweetheart and they have two children together. John is an active philanthropist and serves on the board of several charities. He also enjoys playing golf, traveling, and spending time with his family.'}, {'\\n\\nMarcella:\\nMarcella is a passionate and driven woman originally from Mexico. She moved to the United States as a young adult to pursue her dreams of becoming a successful businesswoman. After graduating from college with a degree in business, Marcella quickly found success in the corporate world. She has held a number of positions in various industries, including finance, marketing, and technology.\\n\\nMarcella is an avid traveler and loves to explore new places. She has visited over 20 countries and has a passion for learning about different cultures and customs. She is also a foodie and loves trying new dishes from around the world.\\n\\nMarcella is a dedicated philanthropist and volunteers her time and resources to various causes. She is an advocate for education and is currently working to bring educational resources to underserved communities in Mexico.\\n\\nIn her free time, Marcella enjoys spending time with her family and friends. She also loves to read, take long walks, and practice yoga. Marcella is an inspiring example of what can be achieved when you set your mind to it.'}, {'\\n\\nSophia Díaz:\\nSophia Díaz was born and raised in the Dominican Republic. She was a bright and ambitious child, excelling in school and dreaming of a better future. She moved to the United States at the age of 18 to pursue her dreams of becoming a successful businesswoman. With hard work and determination, she earned a degree in business administration and began working in the corporate world.\\n\\nSophia quickly rose through the ranks and eventually became the president of a large company. She was a natural leader, inspiring her colleagues and motivating them to reach their highest potential. Sophia was also an advocate for diversity and inclusion in the workplace, and she was instrumental in creating a more equitable work environment.\\n\\nThroughout her career, Sophia was a generous philanthropist, donating her time and money to various causes. She was passionate about improving the lives of those around her and worked to create a more equitable society. Sophia was also a strong advocate for education, believing that it was the key to unlocking one’s potential.\\n\\nSophia Díaz was an inspiring leader and a dedicated philanthropist who touched the lives of many. She was a role model for young women everywhere and an example of what can be achieved with hard work and dedication.'}, {'\\n\\nRajesh Patel: Rajesh Patel was born and raised in Gujarat, India. He grew up in a middle-class family, with his parents instilling in him a strong work ethic and a passion for education. After graduating from high school, Rajesh went on to study engineering at the Indian Institute of Technology. He excelled in his studies and was awarded a scholarship to pursue a postgraduate degree at the University of Oxford in the United Kingdom.\\n\\nRajesh moved to the UK and began his studies, eventually earning a doctorate in engineering. After completing his studies, Rajesh decided to stay in the UK, and he began working as an engineer for a large engineering firm. Over the years, Rajesh worked his way up the corporate ladder and eventually became the CEO of the company.\\n\\nThroughout his career, Rajesh was known for his innovative ideas and his ability to think outside the box. He was also passionate about giving back to his community, and he was a major supporter of educational initiatives in India. Rajesh retired from his career in engineering in 2020, and he currently lives in London with his wife and two children.'}, {\"\\n\\nKatherine Smith:\\nKatherine Smith is a native of Chicago, Illinois. She was born and raised in the city and has always been passionate about the arts. She studied theatre at Northwestern University and graduated with a Bachelor's degree in Theatre Arts. After college, she moved to Los Angeles and began her career in the entertainment industry as an actress and writer. She has appeared in a number of films and television shows, and has written for several television series and films. Katherine is also an avid traveler and has visited many countries around the world. She loves to explore different cultures and has a passion for learning about different cultures. In her spare time, Katherine enjoys spending time with her family and friends, reading, and spending time outdoors.\"}, {\"\\n\\nImani Smith:\\nImani Smith was born in Jamaica and immigrated to the United States at the age of 8. Growing up in a small town in Florida, Imani was determined to make a difference in her community. She worked hard in school, eventually graduating from Florida State University with a degree in Political Science. After college, she moved to Washington D.C. and began working for a non-profit organization that focused on helping those in need.\\n\\nImani was passionate about making a difference and quickly rose through the ranks of the organization. She eventually became the Executive Director, leading the organization to new heights. Imani was a strong advocate for social justice and was instrumental in helping to pass several pieces of legislation that benefited those in need.\\n\\nImani's work in Washington D.C. earned her recognition from several prominent politicians and she was even invited to the White House to meet with President Obama. She was also invited to speak at numerous events, including the United Nations General Assembly.\\n\\nToday, Imani continues to work for the non-profit organization, but also serves on several boards and committees. She is a respected leader in her community and is a role model for many young people. Imani's life story is an inspiration to anyone who dreams of making a difference in the world.\"}, {\"\\n\\nAamir Khan:\\nAamir Khan was born in India and raised in Mumbai. He is a renowned actor, director, producer, and philanthropist. He began his career in the entertainment industry in 1988 with the film Qayamat Se Qayamat Tak, and his performance in the film won him a Filmfare Award for Best Male Debut. He has since gone on to star in some of the most successful films in Bollywood, including Dil Chahta Hai, 3 Idiots, and Dangal.\\n\\nIn addition to his film career, Khan is also involved in several charity initiatives. He is the founder of the NGO Paani Foundation, which works to bring water to rural areas of India. He is also a vocal advocate for women's rights and has spoken out against violence against women. He is a strong supporter of education and has donated to various educational charities in India. Khan has been recognized for his work in the entertainment industry and his philanthropic endeavors, receiving numerous awards and accolades.\"}, {'\\n\\nJohn Smith:\\nJohn Smith was born in London, England in 1975. He was raised in a middle-class family and his parents always encouraged him to pursue his dreams. John was an excellent student and always strived to do his best in school. After graduating from high school, John attended college and eventually earned a degree in business.\\n\\nJohn then went on to work in the banking industry, where he quickly rose through the ranks to become a successful executive. He was always eager to learn and take on new challenges, and his hard work and dedication paid off.\\n\\nJohn eventually decided to pursue his entrepreneurial dreams and started his own business. He worked tirelessly to make it a success and eventually became one of the most successful businessmen in the area. He was well respected in the business community and was an inspiration to many.\\n\\nJohn was also an active member of his community and was involved in many charities and organizations. He was a generous man who always put others before himself. John was married and had two children, and he was a loving father who always put his family first.\\n\\nJohn Smith passed away in 2020, leaving behind a legacy of hard work, dedication, and generosity. He will be remembered fondly by all who knew him.'}, {\"\\n\\nJenna Smith:\\nJenna Smith was born and raised in the small town of Middletown, Ohio. She was the youngest of three children and had a passion for learning from a young age. She graduated from Middletown High School at the top of her class and went on to attend Ohio State University. Jenna was a bright and ambitious student, and she graduated with honors in three years.\\n\\nAfter college, Jenna moved to New York City to pursue her dream of becoming a successful businesswoman. She quickly found success in the corporate world and worked her way up the ladder at an investment firm. Jenna was a hard worker and was quickly promoted to a managerial position.\\n\\nJenna eventually decided to start her own business. She opened a boutique clothing store in the heart of Manhattan and it quickly became a success. Jenna's business acumen and keen eye for fashion soon made her a well-known figure in the fashion industry.\\n\\nToday, Jenna continues to run her business with success. She is an inspiration to many and a role model for women everywhere. Jenna is a living example of how hard work and dedication can lead to success.\"}, {'\\n\\nNina Smith:\\nNina Smith is a native of Los Angeles, California. She grew up in a large family with five siblings, and was always interested in the arts. She attended a prestigious art school and went on to pursue a career in graphic design. After college, Nina moved to New York City and quickly made a name for herself in the design world. She worked with some of the biggest names in the industry, and her designs were featured in major publications.\\n\\nNina is an incredibly talented designer, but she is also passionate about giving back to her community. She volunteers her time and resources to help those in need, and she is an active member of several charities. Her philanthropic efforts have earned her the respect of her peers, and she continues to use her creativity to make the world a better place.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1_1oPRNSW7QWdlUs-APMV5Y7h6RxU_8gF')\n",
        "with open('cs.cl.sample100.json') as f:\n",
        "    data = f.readlines()\n",
        "parsed = [json.loads(x) for x in data]\n",
        "sample10 = random.choices(parsed, k=10)\n",
        "sample10"
      ],
      "metadata": {
        "id": "-wV1yokC74pU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b382b2-39ef-469a-92f1-fb088b0abeaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_1oPRNSW7QWdlUs-APMV5Y7h6RxU_8gF\n",
            "To: /content/cs.cl.sample100.json\n",
            "100%|██████████| 135k/135k [00:00<00:00, 36.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'cs/9810015',\n",
              "  'submitter': 'William Schuler',\n",
              "  'authors': 'Giorgio Satta (Universita di Padova) and William Schuler (University\\n  of Pennsylvania)',\n",
              "  'title': 'Restrictions on Tree Adjoining Languages',\n",
              "  'comments': '7 pages LaTeX + 5 eps figures',\n",
              "  'journal-ref': \"Proceedings of COLING-ACL'98\",\n",
              "  'doi': None,\n",
              "  'report-no': None,\n",
              "  'categories': 'cs.CL',\n",
              "  'license': None,\n",
              "  'abstract': '  Several methods are known for parsing languages generated by Tree Adjoining\\nGrammars (TAGs) in O(n^6) worst case running time. In this paper we investigate\\nwhich restrictions on TAGs and TAG derivations are needed in order to lower\\nthis O(n^6) time complexity, without introducing large runtime constants, and\\nwithout losing any of the generative power needed to capture the syntactic\\nconstructions in natural language that can be handled by unrestricted TAGs. In\\nparticular, we describe an algorithm for parsing a strict subclass of TAG in\\nO(n^5), and attempt to show that this subclass retains enough generative power\\nto make it useful in the general case.\\n',\n",
              "  'versions': [{'version': 'v1', 'created': 'Tue, 13 Oct 1998 21:17:13 GMT'}],\n",
              "  'update_date': '2007-05-23',\n",
              "  'authors_parsed': [['Satta', 'Giorgio', '', 'Universita di Padova'],\n",
              "   ['Schuler', 'William', '', 'University\\n  of Pennsylvania']]},\n",
              " {'id': 'cs/9902029',\n",
              "  'submitter': 'Roberta Catizone',\n",
              "  'authors': 'Yorick Wilks',\n",
              "  'title': 'The \"Fodor\"-FODOR fallacy bites back',\n",
              "  'comments': None,\n",
              "  'journal-ref': None,\n",
              "  'doi': None,\n",
              "  'report-no': 'cs-98-13',\n",
              "  'categories': 'cs.CL',\n",
              "  'license': None,\n",
              "  'abstract': \"  The paper argues that Fodor and Lepore are misguided in their attack on\\nPustejovsky's Generative Lexicon, largely because their argument rests on a\\ntraditional, but implausible and discredited, view of the lexicon on which it\\nis effectively empty of content, a view that stands in the long line of\\nexplaining word meaning (a) by ostension and then (b) explaining it by means of\\na vacuous symbol in a lexicon, often the word itself after typographic\\ntransmogrification. (a) and (b) both share the wrong belief that to a word must\\ncorrespond a simple entity that is its meaning. I then turn to the semantic\\nrules that Pustejovsky uses and argue first that, although they have novel\\nfeatures, they are in a well-established Artificial Intelligence tradition of\\nexplaining meaning by reference to structures that mention other structures\\nassigned to words that may occur in close proximity to the first. It is argued\\nthat Fodor and Lepore's view that there cannot be such rules is without\\nfoundation, and indeed systems using such rules have proved their practical\\nworth in computational systems. Their justification descends from line of\\nargument, whose high points were probably Wittgenstein and Quine that meaning\\nis not to be understood by simple links to the world, ostensive or otherwise,\\nbut by the relationship of whole cultural representational structures to each\\nother and to the world as a whole.\\n\",\n",
              "  'versions': [{'version': 'v1', 'created': 'Thu, 25 Feb 1999 14:41:24 GMT'}],\n",
              "  'update_date': '2007-05-23',\n",
              "  'authors_parsed': [['Wilks', 'Yorick', '']]},\n",
              " {'id': 'cs/9909002',\n",
              "  'submitter': 'Vincenzo Pallotta',\n",
              "  'authors': 'Afzal Ballim and Vincenzo Pallotta',\n",
              "  'title': 'Semantic robust parsing for noun extraction from natural language\\n  queries',\n",
              "  'comments': None,\n",
              "  'journal-ref': \"Proceedings of WPDI'99 (Workshop on Procedures in Discourse\\n  Interpretation),1999, Iasi - Romania\",\n",
              "  'doi': None,\n",
              "  'report-no': None,\n",
              "  'categories': 'cs.CL',\n",
              "  'license': None,\n",
              "  'abstract': '  This paper describes how robust parsing techniques can be fruitful applied\\nfor building a query generation module which is part of a pipelined NLP\\narchitecture aimed at process natural language queries in a restricted domain.\\nWe want to show that semantic robustness represents a key issue in those NLP\\nsystems where it is more likely to have partial and ill-formed utterances due\\nto various factors (e.g. noisy environments, low quality of speech recognition\\nmodules, etc...) and where it is necessary to succeed, even if partially, in\\nextracting some meaningful information.\\n',\n",
              "  'versions': [{'version': 'v1', 'created': 'Thu, 2 Sep 1999 15:53:07 GMT'}],\n",
              "  'update_date': '2007-05-23',\n",
              "  'authors_parsed': [['Ballim', 'Afzal', ''], ['Pallotta', 'Vincenzo', '']]},\n",
              " {'id': 'cs/0408060',\n",
              "  'submitter': 'Docteur Francois Trouilleux',\n",
              "  'authors': 'Gabriel G. Bes (GRIL), Lionel Lamadon (GRIL), Francois Trouilleux\\n  (GRIL)',\n",
              "  'title': 'Verbal chunk extraction in French using limited resources',\n",
              "  'comments': None,\n",
              "  'journal-ref': None,\n",
              "  'doi': None,\n",
              "  'report-no': None,\n",
              "  'categories': 'cs.CL',\n",
              "  'license': None,\n",
              "  'abstract': '  A way of extracting French verbal chunks, inflected and infinitive, is\\nexplored and tested on effective corpus. Declarative morphological and local\\ngrammar rules specifying chunks and some simple contextual structures are used,\\nrelying on limited lexical information and some simple heuristic/statistic\\nproperties obtained from restricted corpora. The specific goals, the\\narchitecture and the formalism of the system, the linguistic information on\\nwhich it relies and the obtained results on effective corpus are presented.\\n',\n",
              "  'versions': [{'version': 'v1', 'created': 'Thu, 26 Aug 2004 12:44:15 GMT'}],\n",
              "  'update_date': '2007-05-23',\n",
              "  'authors_parsed': [['Bes', 'Gabriel G.', '', 'GRIL'],\n",
              "   ['Lamadon', 'Lionel', '', 'GRIL'],\n",
              "   ['Trouilleux', 'Francois', '', 'GRIL']]},\n",
              " {'id': 'cs/9906034',\n",
              "  'submitter': 'Davide Turcato',\n",
              "  'authors': 'Davide Turcato, Paul McFetridge, Fred Popowich, Janine Toole',\n",
              "  'title': 'A Unified Example-Based and Lexicalist Approach to Machine Translation',\n",
              "  'comments': '11 pages, to be presented at the 8th International Conference on\\n  Theoretical and Methodological Issues in Machine Translation (TMI-99)',\n",
              "  'journal-ref': None,\n",
              "  'doi': None,\n",
              "  'report-no': None,\n",
              "  'categories': 'cs.CL',\n",
              "  'license': None,\n",
              "  'abstract': '  We present an approach to Machine Translation that combines the ideas and\\nmethodologies of the Example-Based and Lexicalist theoretical frameworks. The\\napproach has been implemented in a multilingual Machine Translation system.\\n',\n",
              "  'versions': [{'version': 'v1', 'created': 'Wed, 30 Jun 1999 23:06:09 GMT'}],\n",
              "  'update_date': '2007-05-23',\n",
              "  'authors_parsed': [['Turcato', 'Davide', ''],\n",
              "   ['McFetridge', 'Paul', ''],\n",
              "   ['Popowich', 'Fred', ''],\n",
              "   ['Toole', 'Janine', '']]},\n",
              " {'id': 'cs/9902030',\n",
              "  'submitter': 'Roberta Catizone',\n",
              "  'authors': 'Yorick Wilks',\n",
              "  'title': 'Is Word Sense Disambiguation just one more NLP task?',\n",
              "  'comments': None,\n",
              "  'journal-ref': None,\n",
              "  'doi': None,\n",
              "  'report-no': 'cs-98-12',\n",
              "  'categories': 'cs.CL',\n",
              "  'license': None,\n",
              "  'abstract': '  This paper compares the tasks of part-of-speech (POS) tagging and\\nword-sense-tagging or disambiguation (WSD), and argues that the tasks are not\\nrelated by fineness of grain or anything like that, but are quite different\\nkinds of task, particularly becuase there is nothing in POS corresponding to\\nsense novelty. The paper also argues for the reintegration of sub-tasks that\\nare being separated for evaluation\\n',\n",
              "  'versions': [{'version': 'v1', 'created': 'Thu, 25 Feb 1999 14:41:32 GMT'}],\n",
              "  'update_date': '2007-05-23',\n",
              "  'authors_parsed': [['Wilks', 'Yorick', '']]},\n",
              " {'id': 'cs/0510015',\n",
              "  'submitter': 'Laurent Audibert',\n",
              "  'authors': 'Laurent Audibert (DELIC)',\n",
              "  'title': 'Word sense disambiguation criteria: a systematic study',\n",
              "  'comments': None,\n",
              "  'journal-ref': '20th International Conference on Computational Linguistics\\n  (COLING-2004) (2004) pp. 910-916',\n",
              "  'doi': None,\n",
              "  'report-no': None,\n",
              "  'categories': 'cs.CL',\n",
              "  'license': None,\n",
              "  'abstract': '  This article describes the results of a systematic in-depth study of the\\ncriteria used for word sense disambiguation. Our study is based on 60 target\\nwords: 20 nouns, 20 adjectives and 20 verbs. Our results are not always in line\\nwith some practices in the field. For example, we show that omitting\\nnon-content words decreases performance and that bigrams yield better results\\nthan unigrams.\\n',\n",
              "  'versions': [{'version': 'v1', 'created': 'Wed, 5 Oct 2005 14:23:19 GMT'}],\n",
              "  'update_date': '2007-05-23',\n",
              "  'authors_parsed': [['Audibert', 'Laurent', '', 'DELIC']]},\n",
              " {'id': 'cs/0512102',\n",
              "  'submitter': 'Andrij Rovenchak',\n",
              "  'authors': 'Solomija Buk and Andrij Rovenchak',\n",
              "  'title': 'Statistical Parameters of the Novel \"Perekhresni stezhky\" (\"The\\n  Cross-Paths\") by Ivan Franko',\n",
              "  'comments': '11 pages',\n",
              "  'journal-ref': 'Quantitative Linguistics 62: Exact methods in the study of\\n  language and text: dedicated to Professor Gabriel Altmann on the occasion of\\n  his 75th birthday / Ed. by P. Grzybek and R. Kohler (Berlin; New York: de\\n  Gruyter), 39-48 (2007)',\n",
              "  'doi': None,\n",
              "  'report-no': None,\n",
              "  'categories': 'cs.CL',\n",
              "  'license': None,\n",
              "  'abstract': '  In the paper, a complex statistical characteristics of a Ukrainian novel is\\ngiven for the first time. The distribution of word-forms with respect to their\\nsize is studied. The linguistic laws by Zipf-Mandelbrot and Altmann-Menzerath\\nare analyzed.\\n',\n",
              "  'versions': [{'version': 'v1', 'created': 'Wed, 28 Dec 2005 13:45:54 GMT'}],\n",
              "  'update_date': '2008-03-18',\n",
              "  'authors_parsed': [['Buk', 'Solomija', ''], ['Rovenchak', 'Andrij', '']]},\n",
              " {'id': 'cs/9907021',\n",
              "  'submitter': 'Jorg Spilker',\n",
              "  'authors': 'Guenther Goerz, Joerg Spilker, Volker Strom, Hans Weber',\n",
              "  'title': 'Architectural Considerations for Conversational Systems -- The\\n  Verbmobil/INTARC Experience',\n",
              "  'comments': '10 pages, to appear in proceedings of First International Workshop on\\n  Human Computer Conversation, Bellagio, Italy',\n",
              "  'journal-ref': None,\n",
              "  'doi': None,\n",
              "  'report-no': None,\n",
              "  'categories': 'cs.CL',\n",
              "  'license': None,\n",
              "  'abstract': \"  The paper describes the speech to speech translation system INTARC, developed\\nduring the first phase of the Verbmobil project. The general design goals of\\nthe INTARC system architecture were time synchronous processing as well as\\nincrementality and interactivity as a means to achieve a higher degree of\\nrobustness and scalability. Interactivity means that in addition to the\\nbottom-up (in terms of processing levels) data flow the ability to process\\ntop-down restrictions considering the same signal segment for all processing\\nlevels. The construction of INTARC 2.0, which has been operational since fall\\n1996, followed an engineering approach focussing on the integration of symbolic\\n(linguistic) and stochastic (recognition) techniques which led to a\\ngeneralization of the concept of a ``one pass'' beam search.\\n\",\n",
              "  'versions': [{'version': 'v1', 'created': 'Wed, 14 Jul 1999 09:21:16 GMT'}],\n",
              "  'update_date': '2019-08-17',\n",
              "  'authors_parsed': [['Goerz', 'Guenther', ''],\n",
              "   ['Spilker', 'Joerg', ''],\n",
              "   ['Strom', 'Volker', ''],\n",
              "   ['Weber', 'Hans', '']]},\n",
              " {'id': 'cs/9909002',\n",
              "  'submitter': 'Vincenzo Pallotta',\n",
              "  'authors': 'Afzal Ballim and Vincenzo Pallotta',\n",
              "  'title': 'Semantic robust parsing for noun extraction from natural language\\n  queries',\n",
              "  'comments': None,\n",
              "  'journal-ref': \"Proceedings of WPDI'99 (Workshop on Procedures in Discourse\\n  Interpretation),1999, Iasi - Romania\",\n",
              "  'doi': None,\n",
              "  'report-no': None,\n",
              "  'categories': 'cs.CL',\n",
              "  'license': None,\n",
              "  'abstract': '  This paper describes how robust parsing techniques can be fruitful applied\\nfor building a query generation module which is part of a pipelined NLP\\narchitecture aimed at process natural language queries in a restricted domain.\\nWe want to show that semantic robustness represents a key issue in those NLP\\nsystems where it is more likely to have partial and ill-formed utterances due\\nto various factors (e.g. noisy environments, low quality of speech recognition\\nmodules, etc...) and where it is necessary to succeed, even if partially, in\\nextracting some meaningful information.\\n',\n",
              "  'versions': [{'version': 'v1', 'created': 'Thu, 2 Sep 1999 15:53:07 GMT'}],\n",
              "  'update_date': '2007-05-23',\n",
              "  'authors_parsed': [['Ballim', 'Afzal', ''], ['Pallotta', 'Vincenzo', '']]}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6zAI8DtHN1O",
        "outputId": "f269fb29-4705-477b-f62e-06b775806fb4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    }
  ]
}